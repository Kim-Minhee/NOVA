import os, base64
from dotenv import load_dotenv
import google.generativeai as genai
from google.cloud import vision
from google.oauth2 import service_account
from pdf2image import convert_from_path
from nova_func import use_rag, extract_info
import streamlit as st

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
gemeni_api_key = os.getenv('GOOGLE_API_KEY')
vision_api_path = os.getenv('VISION_API_PATH')

# Gemini ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_model():
    genai.configure(api_key=gemeni_api_key)
    model = genai.GenerativeModel('gemini-1.5-pro')
    return model

# Cloud Vision í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ
@st.cache_resource
def load_client():
    try:
        credentials = service_account.Credentials.from_service_account_file(vision_api_path)
        client = vision.ImageAnnotatorClient(credentials=credentials)
        return client
    except Exception as e:
        st.error(f'Error in load_client(): {str(e)}')
        return None
    

### OCR ###
# í…ìŠ¤íŠ¸ ì¶”ì¶œ (image)
def detect_text_from_image(image_path, client):
    try:
        with open(image_path, 'rb') as image_file:
            content = image_file.read()
        
        image = vision.Image(content=content)
        response = client.text_detection(image=image)
        texts = response.text_annotations

        # st.write(texts[0].description)
        return texts[0].description
    except Exception as e:
        st.error(f'Error in detect_text_from_image(): {str(e)}')
        return None

# í…ìŠ¤íŠ¸ ì¶”ì¶œ (pdf)
def detect_text_from_pdf(input_path, save_path, client):
    try:
        pages = convert_from_path(input_path)
        all_text = ''
        for i, page in enumerate(pages):
            image_path = f'{save_path}/{str(i)}.jpg'
            page.save(image_path, 'JPEG')
        
            text = detect_text_from_image(image_path, client)
            all_text += f'\n\n--- Page {i+1} ---\n\n{text}'
        
        return all_text
    except Exception as e:
        st.error(f'Error in detect_text_from_pdf(): {str(e)}')
        return None

# ì—…ë¡œë“œëœ pdf íŒŒì¼ ì²˜ë¦¬
def process_pdf(uploaded_file, pdf_save_dir, jpg_save_dir, client):
    try:
        # pdf íŒŒì¼ ì €ì¥
        pdf_path = os.path.join(pdf_save_dir, uploaded_file.name)
        with open(pdf_path, 'wb') as f:
                f.write(uploaded_file.read())
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        with st.spinner('í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...'):
            jpg_dir = os.path.join(jpg_save_dir, uploaded_file.name)
            os.makedirs(jpg_dir, exist_ok=True)
            extracted_text = detect_text_from_pdf(pdf_path, jpg_dir, client)
        
        return pdf_path, extracted_text
    
    except Exception as e:
        st.error(f'Error in process_pdf(): {str(e)}')
        return None, None

# ì—…ë¡œë“œëœ pdf ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
def display_pdf(pdf_path, extracted_text):
    col1, col2 = st.columns([1, 1], gap='large')
    
    with col1: # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¶œë ¥
        st.text_area('OCR ê²°ê³¼', extracted_text, height=400)
    
    with col2: # ì—…ë¡œë“œëœ pdf ì¶œë ¥
        with open(pdf_path, 'rb') as f:
            pdf_bytes = f.read()
        base64_pdf = base64.b64encode(pdf_bytes).decode('utf-8')
        pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height=400 type="application/pdf"></iframe>'
        st.write('ì—…ë¡œë“œ ëœ ë…¼ë¬¸ íŒŒì¼')
        st.markdown(pdf_display, unsafe_allow_html=True)

# ì±„íŒ… ë‚´ì—­ì— pdf ì²˜ë¦¬ ê²°ê³¼ ì¶”ê°€
def add_pdf_to_chat(filename, title, abstract, conclusion):
    system_message = f'''ìƒˆë¡œìš´ PDF íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {filename}
    
ë…¼ë¬¸ ì œëª©

{title}

ë…¼ë¬¸ ìš”ì•½

{abstract}

ë…¼ë¬¸ ê²°ë¡ 

{conclusion}
'''
    
    st.session_state.messages.append({'role': 'assistant', 'content': system_message})

def generate_combined_prompt(rag_response, user_query, chat_history):

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
    formatted_history = '\n'.join([f'{"User" if msg["role"]=="user" else "Assistant"}: {msg["content"]}' for msg in chat_history[:-1]])
    if rag_response:
        # RAGì—ì„œ ì°¾ì€ ì •ë³´ê°€ ìˆìœ¼ë©´ ë…¼ë¬¸ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        combined_rag_response = f"""
        ì œëª©: {rag_response.get('Title', 'ì •ë³´ ì—†ìŒ')}
        ìš”ì•½: {rag_response.get('Abstract', 'ì •ë³´ ì—†ìŒ')}
        ê²°ë¡ : {rag_response.get('Conclusion', 'ì •ë³´ ì—†ìŒ')}
        """
    else:
        # RAG ì‘ë‹µì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
        combined_rag_response = ""

    # ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì§ˆë¬¸ì„ ê²°í•©
    context_prompt = f"""
    ì´ì „ ëŒ€í™” ë‚´ìš©:
    {formatted_history}

    í˜„ì¬ ì§ˆë¬¸:
    {user_query}

    RAGë¡œ ì°¾ì€ ë…¼ë¬¸ ì •ë³´:
    {combined_rag_response}
    """

    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
    final_prompt = f"""
    [context]: {context_prompt}
    ---
    ìœ„ì˜ [context] ì •ë³´ì™€ ì´ì „ ëŒ€í™”ì™€ ë§¥ë½ì´ ë¹„ìŠ·í•˜ë©´ í™œìš© í•˜ê³  ì•„ë‹ˆë©´ ìƒˆë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
    ë§Œì•½, ì§ˆë¬¸ì— RAGë¡œ ì°¾ì€ ë…¼ë¬¸ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ ì¤„ í•„ìš”ê°€ ìˆë‹¤ë©´ :stars:ë¥¼ ëŒ€ë‹µì— ì¶”ê°€í•´ì£¼ì„¸ìš”.
    """
    
    return final_prompt
### Chatbot ###
# geminiì™€ ëŒ€í™”í•˜ê¸°
def chat_with_gemini(model, prompt,dict_response):
    try:
        response = model.generate_content(prompt)
        
        if dict_response and ":stars:" in response.text:
            arxiv_id = dict_response.get('arXiv_id', None)
            if arxiv_id:
                # pdf_file_path ê²½ë¡œ ì„¤ì •
                pdf_file_path = f"nova/papers/{arxiv_id}v1.pdf"  # ë…¼ë¬¸ PDF íŒŒì¼ ê²½ë¡œ
                
                # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
                if os.path.exists(pdf_file_path):
                    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ìƒì„±
                    st.download_button(
                        label="ğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ",
                        data=open(pdf_file_path, "rb").read(),
                        file_name=f"{arxiv_id}v1.pdf",
                        mime="application/pdf"
                    )
            rag_info = f"""
            ğŸ” ì°¾ì€ ë…¼ë¬¸ ğŸ”

        ì œëª©: {dict_response.get('Title', 'ì •ë³´ ì—†ìŒ')}
        ìš”ì•½: {dict_response.get('Abstract', 'ì •ë³´ ì—†ìŒ')}
        ê²°ë¡ : {dict_response.get('Conclusion', 'ì •ë³´ ì—†ìŒ')}
            """
            # ìµœì¢… ë‹µë³€ì— ë…¼ë¬¸ ì •ë³´ í¬í•¨
            full_response = f"{response.text}\n\n{rag_info}"
            return full_response
        else:
            return response.text       
    except Exception as e:
        st.error(f'Error in chat_with_gemini(): {str(e)}')
        return None

# ì±„íŒ… ì¶œë ¥
def display_chat(model):
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'messages' not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append({ # ì´ˆê¸° ë©”ì‹œì§€
            'role': 'assistant',
            'content': 'ì•ˆë…•í•˜ì„¸ìš”ğŸ¤— AI ë…¼ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ Novaì…ë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'
        })
    if 'current_pdf' not in st.session_state:
        st.session_state.current_pdf = None

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message['role'], avatar="ğŸ§™â€â™‚ï¸" if message['role'] == 'assistant' else "ğŸ¬"):
                st.markdown(message['content'])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt:=st.chat_input('Novaì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”.'):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({'role': 'user', 'content': prompt})
        with st.chat_message('user',avatar="ğŸ¬"):
            st.markdown(prompt)
        
        # gemini ì‘ë‹µ
        with st.chat_message('assistant',avatar="ğŸ§™â€â™‚ï¸"):
            dict_response = use_rag(prompt)
            final_prompt = generate_combined_prompt(dict_response, prompt, st.session_state.messages)
            response = chat_with_gemini(model, final_prompt,dict_response) 
            st.markdown(response)
            st.session_state.messages.append({'role': 'assistant', 'content': response})

### UI_Streamlit ###
def main():
    st.set_page_config(
        page_title='Nova',
        page_icon='img/nova_icon.png',
        layout='wide')
    st.title('NOVAì™€ ëŒ€í™”í•˜ê¸°')

    # ëª¨ë¸ê³¼ í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ
    model = load_model()
    client = load_client()
    
    ## OCR ì„¹ì…˜ ##
    # ì‚¬ì´ë“œ ë°” (íŒŒì¼ ì—…ë¡œë“œ)
    st.sidebar.header('ë…¼ë¬¸ íŒŒì¼ ì—…ë¡œë“œ')
    uploaded_file = st.sidebar.file_uploader('ë…¼ë¬¸ íŒŒì¼(PDF)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.', type=['pdf'])

    # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    # pdf_save_dir, jpg_save dir ê²½ë¡œ ì„¤ì • í•„ìš”
    pdf_save_dir = 'nova/input_pdf'
    jpg_save_dir = 'nova/input_pdf/save_jpg'
    os.makedirs(pdf_save_dir, exist_ok=True)

    # ìƒˆë¡œìš´ PDF íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš°
    if uploaded_file is not None and uploaded_file.name!=st.session_state.current_pdf:
        pdf_path, extracted_text = process_pdf(uploaded_file, pdf_save_dir, jpg_save_dir, client)
        title, abstract, conclusion = extract_info(extracted_text)

        if pdf_path and extracted_text:
            display_pdf(pdf_path, extracted_text)
            add_pdf_to_chat(uploaded_file.name, title, abstract, conclusion)
            st.session_state.current_pdf = uploaded_file.name
    
    # ì´ì „ì— ì—…ë¡œë“œëœ PDF íŒŒì¼ ë‹¤ì‹œ í‘œì‹œ
    elif uploaded_file is not None:
        pdf_path = os.path.join(pdf_save_dir, uploaded_file.name)
        extracted_text = detect_text_from_pdf(pdf_path, os.path.join(jpg_save_dir, uploaded_file.name), client)
        display_pdf(pdf_path, extracted_text)
    
    st.markdown('---')

    ## ì±—ë´‡ ì„¹ì…˜ ##
    display_chat(model)

if __name__=='__main__':
    main()